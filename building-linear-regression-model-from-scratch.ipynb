{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9018903,"sourceType":"datasetVersion","datasetId":5434697}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T15:39:20.565165Z","iopub.execute_input":"2024-07-23T15:39:20.565591Z","iopub.status.idle":"2024-07-23T15:39:21.969740Z","shell.execute_reply.started":"2024-07-23T15:39:20.565560Z","shell.execute_reply":"2024-07-23T15:39:21.968452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/person-salary/salary_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:40:18.242657Z","iopub.execute_input":"2024-07-23T15:40:18.243104Z","iopub.status.idle":"2024-07-23T15:40:18.268569Z","shell.execute_reply.started":"2024-07-23T15:40:18.243069Z","shell.execute_reply":"2024-07-23T15:40:18.267292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-23T15:40:28.355056Z","iopub.execute_input":"2024-07-23T15:40:28.355554Z","iopub.status.idle":"2024-07-23T15:40:28.375915Z","shell.execute_reply.started":"2024-07-23T15:40:28.355515Z","shell.execute_reply":"2024-07-23T15:40:28.374801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class linear_regression():\n\n    # initiating the parameters (learning rate , no of iterations )\n    def __init__(self,learning_rate,no_of_iterations):\n        \n        self.learning_rate = learning_rate\n        self.no_of_iterations = no_of_iterations\n    \n    def fit(self, X , Y): #X and Y is nothing but the data we have year_of_experience and salary \n        # number of training examples and number of features \n        \n        self.m ,self.n = X.shape #number of rows and columns (m-->> rows and n--> columns)\n        \n        #intiating the weight and bias \n        \n        self.w = np.zeros(self.n) # here we are creating an matrix of array with n no. of columns which contain all values as zero \n        self.b = 0 \n        self.X = X\n        self.Y = Y\n        \n        #implementing gradient descent \n        \n        for i in range(self.no_of_iterations):\n            self.update_weights()\n        \n    \n    \n    def update_weights(self,):\n        Y_prediction = self.predict(self.X)\n        \n        #calculate gradients \n        \n        dw = - (2*(self.X.T).dot(self.Y - Y_prediction))/self.m\n\n        db = - (2*np.sum(self.Y - Y_prediction))/self.m\n        \n        #updating the weights \n        \n        self.w = self.w - self.learning_rate*dw\n        self.b = self.b - self.learning_rate*db\n        \n    \n    def predict(self,X):\n        return X.dot(self.w) + self.b\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T17:08:23.172324Z","iopub.execute_input":"2024-07-23T17:08:23.172770Z","iopub.status.idle":"2024-07-23T17:08:23.184100Z","shell.execute_reply.started":"2024-07-23T17:08:23.172737Z","shell.execute_reply":"2024-07-23T17:08:23.182817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()","metadata":{},"execution_count":null,"outputs":[]}]}